{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION ONE\n",
    "Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "|               | actual cat | actual dog |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| predicted cat |         34 |          7 |\n",
    "| predicted dog |         13 |         46 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of this problem, what is a false positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will define success as accurately identifying a cat\n",
    "# A false positive is when cat is predicted, but the actual was a dog (7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of this problem, what is a false negative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A false negative is when dog is predicted, but the actual is a cat (13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would you describe this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is 80.0%\n",
      "Model PPV is 82.93%\n",
      "Model NPV is 77.97%\n",
      "Model Recall(Sensitivity) is 72.34%\n",
      "Model Specificity is 86.79%\n"
     ]
    }
   ],
   "source": [
    "# Overall success rate is 34+46 / 34+7+13+46\n",
    "print(f'Model accuracy is {(34+46)/(34+7+13+46) * 100}%') # Given a prediction, how much do we trust that prediction?\n",
    "print(f'Model PPV is {round(34/(34+7)*100, 2)}%') # Given a positive prediction, how much do we trust that prediction?\n",
    "print(f'Model NPV is {round(46/(13+46)*100,2)}%') # Given a negative prediction, how much do we trust that negative prediction?\n",
    "print(f'Model Recall(Sensitivity) is {round(34/(34+13)*100, 2)}%') # How well is this model minimizing false negatives?\n",
    "print(f'Model Specificity is {round(46/(7+46)*100,2)}%') # How well is this model minimizing false positives? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION TWO\n",
    "You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found here.\n",
    "\n",
    "Use the predictions dataset and pandas to help answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3\n",
       "0  No Defect  No Defect  Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect\n",
       "2  No Defect  No Defect  Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect\n",
       "4  No Defect  No Defect  Defect  No Defect"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('c3.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 4)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No Defect    184\n",
       "Defect        16\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 1 Accuracy\n",
    "(df.actual == df.model1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 2 Accuracy\n",
    "(df.actual == df.model2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.555"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 3 Accuracy\n",
    "(df.actual == df.model3).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?\n",
    "\n",
    "The team is less concerned about accuracy, and more concerned about catching as many defects as possible.\n",
    "We first need to define what our positive and negative conditions are.\n",
    "Because the goal of the model is to identify defective ducks, we will define a positive result as a defect.\n",
    "\n",
    "With this in mind, we need to minimize false negatives (a defective duck slips through)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>8</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual     Defect  No Defect\n",
       "model1                      \n",
       "Defect          8          2\n",
       "No Defect       8        182"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df.model1, df.actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>9</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>7</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual     Defect  No Defect\n",
       "model2                      \n",
       "Defect          9         81\n",
       "No Defect       7        103"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df.model2, df.actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>13</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual     Defect  No Defect\n",
       "model3                      \n",
       "Defect         13         86\n",
       "No Defect       3         98"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df.model3, df.actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although model3 has the worst overall accuracy, it has the best recall(sensitivity). The fewest number of defective ducks will slip through this model compared to the other models. The company will investigate a lot of good ducks, but they will have the best chance of finding the highest number of defects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?\n",
    "\n",
    "The PR team wants to minimize False positives (where the prediction is that the duck was defective, but the duck was actually good). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>8</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual     Defect  No Defect\n",
       "model1                      \n",
       "Defect          8          2\n",
       "No Defect       8        182"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df.model1, df.actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 1 reduces the number of False positives (high specificity). Only 10 customers will recieve vacation packages, and out of that 10, only 2 customers didn't actually deserve them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUESTION THREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are working as a data scientist for Gives You Paws â„¢, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).\n",
    "\n",
    "At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. First an automated algorithm tags pictures as either a cat or a dog (Phase I). Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II).\n",
    "\n",
    "Several models have already been developed with the data, and you can find their results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog\n",
       "2    dog    cat    cat    cat    dog\n",
       "3    dog    dog    dog    cat    dog\n",
       "4    cat    cat    cat    dog    dog"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('gives_you_paws.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dog    3254\n",
       "cat    1746\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline accuracy (always predicting dog) is 65.08%\n"
     ]
    }
   ],
   "source": [
    "baseline_accuracy = (df.actual == 'dog').mean()\n",
    "print(f'The baseline accuracy (always predicting dog) is {baseline_accuracy * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.model1 == df.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6304"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.model2 == df.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5096"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.model3 == df.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7426"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.model4 == df.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 has the highest accuracy at 80.74%\n",
    "# Model 4 has the next highest accuracy at 74.26%\n",
    "# Model 2 & 3 performed worse than the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recomend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1423</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>323</td>\n",
       "      <td>2614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model1            \n",
       "cat     1423   640\n",
       "dog      323  2614"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df.model1, df.actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL ONE\n",
      "Each actual DOG has a 0.803318992009834 percent chance of being tagged as dog.\n",
      "Each actual CAT has a 0.1849942726231386 percent chance of being tagged as dog.\n"
     ]
    }
   ],
   "source": [
    "print('MODEL ONE')\n",
    "print(f'Each actual DOG has a {2614/(2614+640)} percent chance of being tagged as dog.')\n",
    "print(f'Each actual CAT has a {323/(323+1423)} percent chance of being tagged as dog.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1555</td>\n",
       "      <td>1657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>191</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model2            \n",
       "cat     1555  1657\n",
       "dog      191  1597"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df.model2, df.actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL TWO\n",
      "Each actual DOG has a 0.49078057775046097 percent chance of being tagged as dog.\n",
      "Each actual CAT has a 0.10939289805269187 percent chance of being tagged as dog.\n"
     ]
    }
   ],
   "source": [
    "print('MODEL TWO')\n",
    "print(f'Each actual DOG has a {1597/(1597+1657)} percent chance of being tagged as dog.')\n",
    "print(f'Each actual CAT has a {191/(191+1555)} percent chance of being tagged as dog.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>893</td>\n",
       "      <td>1599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>853</td>\n",
       "      <td>1655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual  cat   dog\n",
       "model3           \n",
       "cat     893  1599\n",
       "dog     853  1655"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df.model3, df.actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL THREE\n",
      "Each actual DOG has a 0.5086047940995697 percent chance of being tagged as dog.\n",
      "Each actual CAT has a 0.488545246277205 percent chance of being tagged as dog.\n"
     ]
    }
   ],
   "source": [
    "print('MODEL THREE')\n",
    "print(f'Each actual DOG has a {1655/(1655+1599)} percent chance of being tagged as dog.')\n",
    "print(f'Each actual CAT has a {853/(853+893)} percent chance of being tagged as dog.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>603</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>1143</td>\n",
       "      <td>3110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model4            \n",
       "cat      603   144\n",
       "dog     1143  3110"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(df.model4, df.actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL FOUR\n",
      "Each actual DOG has a 0.9557467732022127 percent chance of being tagged as dog.\n",
      "Each actual CAT has a 0.654639175257732 percent chance of being tagged as dog.\n"
     ]
    }
   ],
   "source": [
    "print('MODEL FOUR')\n",
    "print(f'Each actual DOG has a {3110/(3110+144)} percent chance of being tagged as dog.')\n",
    "print(f'Each actual CAT has a {1143/(1143+603)} percent chance of being tagged as dog.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog    2937\n",
      "cat    2063\n",
      "Name: model1, dtype: int64\n",
      "cat    3212\n",
      "dog    1788\n",
      "Name: model2, dtype: int64\n",
      "dog    2508\n",
      "cat    2492\n",
      "Name: model3, dtype: int64\n",
      "dog    4253\n",
      "cat     747\n",
      "Name: model4, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# If I wanted to do the least amount of work, I would want the model that predicted the fewest number of dogs, regardless of any other metric\n",
    "print(df.model1.value_counts())\n",
    "print(df.model2.value_counts())\n",
    "print(df.model3.value_counts())\n",
    "print(df.model4.value_counts())\n",
    "\n",
    "# Model 3 would send the least amount of work to my team, so lets start with that one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just kidding...lets look at the possible combinations of the four models we have and what their expected combined performance is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Model 2\n",
    "If we use model 1, what do we pass to phase II?\n",
    "\n",
    "    2614 dogs, 323 cats\n",
    "    \n",
    "If we use model 2, what do we pass to the users?\n",
    "\n",
    "    Each actual dog has a 0.49078057775046097 chance of being passed to user\n",
    "    Each actual cat has a 0.10939289805269187 chance of being passed to user\n",
    "   \n",
    "We pass on 1283 dogs and 35 cats\n",
    "\n",
    "#### Accuracy = 97.34%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Model 1\n",
    "If we use model 2, what do we pass to phase II?\n",
    "\n",
    "    1597 dogs, 191 cats\n",
    "    \n",
    "If we use model 1, what do we pass to the users?\n",
    "\n",
    "    Each actual DOG has a 0.803318992009834 percent chance of being tagged as dog.\n",
    "    Each actual CAT has a 0.1849942726231386 percent chance of being tagged as dog.\n",
    "    \n",
    "We pass on 1283 dogs, 35 cats\n",
    "\n",
    "#### Accuracy: 97.34%\n",
    "\n",
    "#### Note: It doesn't matter which test is run first, the end result is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Model 3\n",
    "If we use model 1, what do we pass to phase II?\n",
    "\n",
    "    2614 dogs, 323 cats\n",
    "    \n",
    "If we use model 3, what do we pass to the users?\n",
    "\n",
    "    Each actual DOG has a 0.5086047940995697 percent chance of being tagged as dog.\n",
    "    Each actual CAT has a 0.488545246277205 percent chance of being tagged as dog.\n",
    "\n",
    "We pass on 653 dogs and 158 cats\n",
    "\n",
    "#### Accuracy = 80.52%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Model 4\n",
    "If we use model 1, what do we pass to phase II?\n",
    "\n",
    "    2614 dogs, 323 cats\n",
    "    \n",
    "If we use model 4, what do we pass to the users?\n",
    "    \n",
    "    Each actual DOG has a 0.9557467732022127 percent chance of being tagged as dog.\n",
    "    Each actual CAT has a 0.654639175257732 percent chance of being tagged as dog.\n",
    "\n",
    "We pass on 2498 dogs and 211 cats\n",
    "\n",
    "#### Accuracy = 92.21%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Model 3\n",
    "If we use model 2, what do we pass to phase II?\n",
    "\n",
    "    1597 dogs, 191 cats\n",
    "    \n",
    "If we use model 3, what do we pass to the users?\n",
    "        \n",
    "        Each actual DOG has a 0.5086047940995697 percent chance of being tagged as dog.\n",
    "        Each actual CAT has a 0.488545246277205 percent chance of being tagged as dog.\n",
    "\n",
    "We pass on 812 dogs, 93 cats\n",
    "\n",
    "#### Accuracy: 89.72%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Model 4\n",
    "If we use model 2, what do we pass to phase II?\n",
    "\n",
    "    1597 dogs, 191 cats\n",
    "    \n",
    "If we use model 4, what do we pass to the users?\n",
    "\n",
    "    Each actual DOG has a 0.9557467732022127 percent chance of being tagged as dog.\n",
    "    Each actual CAT has a 0.654639175257732 percent chance of being tagged as dog.\n",
    "    \n",
    "We pass on 1526 dogs, 125 cats\n",
    "\n",
    "#### Accuracy: 92.43%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Model 4\n",
    "If we use model 3, what do we pass to phase II?\n",
    "\n",
    "    1655 dogs, 853 cats\n",
    "    \n",
    "If we use model 4, what do we pass to the users?\n",
    "    \n",
    "    Each actual DOG has a 0.9557467732022127 percent chance of being tagged as dog.\n",
    "    Each actual CAT has a 0.654639175257732 percent chance of being tagged as dog.\n",
    "    \n",
    "We pass on 1582 dogs, 558 cats\n",
    "\n",
    "#### Accuracy: 73.93%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Combination</th>\n",
       "      <th>Dogs Passed to User</th>\n",
       "      <th>Cats Passed to User</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Model 1:2</td>\n",
       "      <td>1283</td>\n",
       "      <td>35</td>\n",
       "      <td>0.9734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Model 1:3</td>\n",
       "      <td>653</td>\n",
       "      <td>158</td>\n",
       "      <td>0.8052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Model 1:4</td>\n",
       "      <td>2498</td>\n",
       "      <td>211</td>\n",
       "      <td>0.9221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Model 2:3</td>\n",
       "      <td>812</td>\n",
       "      <td>93</td>\n",
       "      <td>0.8972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Model 2:4</td>\n",
       "      <td>1526</td>\n",
       "      <td>125</td>\n",
       "      <td>0.9243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Model 3:4</td>\n",
       "      <td>1582</td>\n",
       "      <td>558</td>\n",
       "      <td>0.7393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Model Combination  Dogs Passed to User  Cats Passed to User  Accuracy\n",
       "0         Model 1:2                 1283                   35    0.9734\n",
       "1         Model 1:3                  653                  158    0.8052\n",
       "2         Model 1:4                 2498                  211    0.9221\n",
       "3         Model 2:3                  812                   93    0.8972\n",
       "4         Model 2:4                 1526                  125    0.9243\n",
       "5         Model 3:4                 1582                  558    0.7393"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_combinations = {'Model Combination':  ['Model 1:2', 'Model 1:3', 'Model 1:4', 'Model 2:3', 'Model 2:4', 'Model 3:4'],\n",
    "        'Dogs Passed to User': [1283, 653, 2498, 812, 1526, 1582],\n",
    "        'Cats Passed to User': [35, 158, 211, 93, 125, 558],\n",
    "        'Accuracy': [.9734, .8052, .9221, .8972, .9243, .7393]\n",
    "        }\n",
    "\n",
    "model_df = pd.DataFrame (model_combinations, columns = ['Model Combination','Dogs Passed to User', 'Cats Passed to User', 'Accuracy'])\n",
    "model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its clear from our results that the combination of models 1 and 2 give the highest possible accuracy. \n",
    "\n",
    "However, there can be a case made for models 1 and 4. While accuracy drops from 97% to 92%, the number of pictures more than doubles. The user is getting many more dog pictures for the same subscription price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recomend for Phase I? For Phase II?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than recalculating every possible model combination for cats, lets consider what characteristics made the combination of models 1 and 2 the most accurate overall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
